Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	2	distance
	2	generate_genome
	5

[Tue Apr 20 17:13:15 2021]
rule generate_genome:
    output: data/GCF/genome/refTSS.tsv
    jobid: 6
    wildcards: genome=GCF

[Tue Apr 20 17:13:40 2021]
Finished job 6.
1 of 5 steps (20%) done

[Tue Apr 20 17:13:40 2021]
rule distance:
    input: data/GCF/anno/peaks.merge.tss.minimal.anno, data/GCF/genome/refTSS.tsv
    output: data/GCF/anno/peaks.distance.tsv
    jobid: 2
    wildcards: genome=GCF

[Tue Apr 20 17:13:43 2021]
Error in rule distance:
    jobid: 2
    output: data/GCF/anno/peaks.distance.tsv
    shell:
        python peak_distance.py distance data/GCF/anno/peaks.merge.tss.minimal.anno data/GCF/genome/refTSS.tsv data/GCF/anno/peaks.distance.tsv --cpu 32
        (exited with non-zero exit code)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /data/isshamie/TSS/Analysis/TSS_code/tss/manuscript/.snakemake/log/2021-04-20T171314.925031.snakemake.log
