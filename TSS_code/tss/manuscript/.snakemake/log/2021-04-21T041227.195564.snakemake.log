Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	addGeneBody
	1	all
	2	create_totalTSS
	1	filter_out_intron
	1	peak_scoring
	1	tissue_merging
	7
Resources before job selection: {'_cores': 1, '_nodes': 9223372036854775807}
Ready jobs (3):
	tissue_merging
	filter_out_intron
	peak_scoring
Selected jobs (1):
	filter_out_intron
Resources after job selection: {'_cores': 0, '_nodes': 9223372036854775806}

[Wed Apr 21 04:12:28 2021]
rule filter_out_intron:
    input: data/synapse/anno/peaks.merge.tss.minimal.anno, data/synapse/genome/introns.bed
    output: data/synapse/filters_genoebody/peaks.nointron.index
    jobid: 21
    wildcards: genome=synapse

[Wed Apr 21 04:12:35 2021]
Finished job 21.
1 of 7 steps (14%) done
Resources before job selection: {'_cores': 1, '_nodes': 9223372036854775807}
Ready jobs (3):
	tissue_merging
	addGeneBody
	peak_scoring
Selected jobs (1):
	addGeneBody
Resources after job selection: {'_cores': 0, '_nodes': 9223372036854775806}

[Wed Apr 21 04:12:35 2021]
rule addGeneBody:
    input: data/synapse/filters_genoebody/peaks.nocds.index, data/synapse/filters_genoebody/peaks.nointron.index, data/synapse/refType/peaks.refType.tsv
    output: data/synapse/refType/peaks.refType.geneBody.tsv
    jobid: 15
    wildcards: genome=synapse

[Wed Apr 21 04:12:42 2021]
Finished job 15.
2 of 7 steps (29%) done
Resources before job selection: {'_cores': 1, '_nodes': 9223372036854775807}
Ready jobs (2):
	tissue_merging
	peak_scoring
Selected jobs (1):
	tissue_merging
Resources after job selection: {'_cores': 0, '_nodes': 9223372036854775806}

[Wed Apr 21 04:12:42 2021]
rule tissue_merging:
    output: data/peaksInfo/tissuesMerge.thresh1.peaksexpression, data/peaksInfo/tissues_scoring.thresh1.tsv
    jobid: 6
    wildcards: thresh=1

[Wed Apr 21 04:12:52 2021]
Finished job 6.
3 of 7 steps (43%) done
Resources before job selection: {'_cores': 1, '_nodes': 9223372036854775807}
Ready jobs (1):
	peak_scoring
Selected jobs (1):
	peak_scoring
Resources after job selection: {'_cores': 0, '_nodes': 9223372036854775806}

[Wed Apr 21 04:12:52 2021]
rule peak_scoring:
    output: data/peaksInfo/peaks_scoring.thresh1.tsv
    jobid: 3
    wildcards: thresh=1

[Wed Apr 21 04:12:57 2021]
Finished job 3.
4 of 7 steps (57%) done
Resources before job selection: {'_cores': 1, '_nodes': 9223372036854775807}
Ready jobs (2):
	create_totalTSS
	create_totalTSS
Selected jobs (1):
	create_totalTSS
Resources after job selection: {'_cores': 0, '_nodes': 9223372036854775806}

[Wed Apr 21 04:12:57 2021]
rule create_totalTSS:
    input: data/peaksInfo/peaks_scoring.thresh1.tsv, data/peaksInfo/peaks_scoring.thresh1.tsv, data/GCF/refType/peaks.refType.geneBody.tsv
    output: data/GCF/allTSS/thresh1/peaksSankey.csv, data/GCF/allTSS/thresh1/peaksSankey.png
    jobid: 8
    wildcards: genome=GCF, thresh=1

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /data/isshamie/TSS/Analysis/TSS_code/tss/manuscript/.snakemake/log/2021-04-21T041227.195564.snakemake.log
unlocking
removing lock
removing lock
removed all locks
